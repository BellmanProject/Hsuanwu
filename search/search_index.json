{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hsuanwu - Overview","text":"<p>Hsuanwu: Long-Term Evolution Project of Reinforcement Learning is inspired by the long-term evolution (LTE) standard project in telecommunications, which aims to track the latest research progress in reinforcement learning (RL) and provide stable baselines. The highlight features of Hsuanwu:</p> <ul> <li>\ud83e\uddf1 Complete decoupling of RL algorithms, and each method can be invoked separately;</li> <li>\ud83d\udcda Large number of reusable bechmarking implementations (See Benchmarks);</li> <li>\ud83d\udee0\ufe0f Support for RL model engineering deployment (C++ API);</li> <li>\ud83d\ude80 Minimizing the CPU to GPU data transferring to realize full GPU-acceleration.</li> </ul> <p>See the project structure below:</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Find all the benchmark results in https://benchmark.hsuanwu.dev/.</p>"},{"location":"changelog/","title":"02/2023","text":""},{"location":"getting_started/","title":"Get Started","text":""},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#with-pip-recommended","title":"with pip recommended","text":"<p>Hsuanwu has been published as a Python package in PyPi and can be installed with <code>pip</code>, ideally by using a virtual environment. Open up a terminal and install Hsuanwu with:</p> <pre><code>pip install hsuanwu\n</code></pre>"},{"location":"getting_started/#with-git","title":"with git","text":"<p>Open up a terminal and clone the repository from GitHub witg <code>git</code>: <pre><code>git clone https://github.com/BellmanProject/Hsuanwu.git\n</code></pre> After that, run the following command to install package and dependencies: <pre><code>pip install -e .\n</code></pre></p> <p>Tip</p> <p>For PyTorch installation, please refer to Get Started and ensure that the right version is installed!  We will also keep providing support for the latest PyTorch version.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 Bellman Project</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"Index","text":"<p>Hsuanwu: Long-Term Evolution Project of Reinforcement Learning is inspired by the long-term evolution (LTE) standard project in telecommunications, which aims to track the latest research progress in reinforcement learning (RL) and provide stable baselines. The highlight features of Hsuanwu:</p> <ul> <li>\ud83e\uddf1 Complete decoupling of RL algorithms, and each method can be invoked separately;</li> <li>\ud83d\udcda Large number of reusable bechmarking implementations (See Benchmarks);</li> <li>\ud83d\udee0\ufe0f Support for RL model engineering deployment (C++ API);</li> <li>\ud83d\ude80 Minimizing the CPU to GPU data transferring to realize full GPU-acceleration.</li> </ul> <p>See the project structure below:</p>"},{"location":"api/examples/example_drqv2_dmc/","title":"Example drqv2 dmc","text":"<p>```python import os import sys</p> <p>curren_dir_path = os.path.dirname(os.path.realpath(file)) parent_dir_path = os.path.abspath(os.path.join(curren_dir_path, os.pardir)) sys.path.append(parent_dir_path)</p> <p>from hsuanwu.env import make_dmc_env from hsuanwu.common.engine import OffPolicyTrainer</p> <p>import hydra</p> <p>train_env = make_dmc_env(domain_name='cartpole',                         task_name='balance',                         resource_files=None,                         img_source=None,                        total_frames=None,                        seed=1,                         visualize_reward=False,                         from_pixels=True,                         frame_skip=2, frame_stack=3)</p> <p>test_env = make_dmc_env(domain_name='cartpole',                         task_name='balance',                         resource_files=None,                         img_source=None,                        total_frames=None,                        seed=1,                         visualize_reward=False,                         from_pixels=True,                         frame_skip=2, frame_stack=3)</p> <p>@hydra.main(version_base=None, config_path='../cfgs', config_name='config') def main(cfgs):     trainer = OffPolicyTrainer(train_env=train_env, test_env=test_env, cfgs=cfgs)     trainer.train()</p> <p>if name == 'main':     main()```</p>"},{"location":"api/xploit/encoder/base/","title":"Base","text":""},{"location":"api/xploit/encoder/base/#baseencoder","title":"BaseEncoder","text":"<p>source <pre><code>BaseEncoder(\nobservation_space: Space, feature_dim: int = 0\n)\n</code></pre></p> <p>Base class that represents a features extractor.</p> <p>Args</p> <ul> <li>observation_space  : Observation space of the environment.</li> <li>feature_dim  : Number of features extracted.</li> </ul> <p>Returns</p> <p>The base encoder class</p> <p>Methods:</p>"},{"location":"api/xploit/encoder/base/#feature_dim","title":".feature_dim","text":"<p>source <pre><code>.feature_dim()\n</code></pre></p>"},{"location":"api/xploit/encoder/vanilla_cnn_encoder/","title":"VanillaCnnEncoder","text":""},{"location":"api/xploit/encoder/vanilla_cnn_encoder/#vanillacnnencoder","title":"VanillaCnnEncoder","text":"<p>source <pre><code>VanillaCnnEncoder(\nobservation_space: Space, feature_dim: int = 64\n)\n</code></pre></p> <p>Convolutional neural network (CNN)-based encoder for processing image-based observations.</p> <p>Args</p> <ul> <li>observation_space  : Observation space of the environment.</li> <li>feature_dim  : Number of features extracted.</li> </ul> <p>Returns</p> <p>CNN-based encoder.</p> <p>Methods:</p>"},{"location":"api/xploit/encoder/vanilla_cnn_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xploit/encoder/vanilla_mlp_encoder/","title":"VanillaMlpEncoder","text":""},{"location":"api/xploit/encoder/vanilla_mlp_encoder/#vanillamlpencoder","title":"VanillaMlpEncoder","text":"<p>source <pre><code>VanillaMlpEncoder(\nobservation_space: Space, feature_dim: int = 64, hidden_dim: int = 256\n)\n</code></pre></p> <p>Multi layer perceptron (MLP) for processing state-based inputs.</p> <p>Args</p> <ul> <li>observation_space  : Observation space of the environment.</li> <li>feature_dim  : Number of features extracted.</li> <li>hidden_dim  : Number of units per hidden layer.</li> </ul> <p>Returns</p> <p>Mlp-based encoder.</p> <p>Methods:</p>"},{"location":"api/xploit/encoder/vanilla_mlp_encoder/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/","title":"DrQv2Agent","text":""},{"location":"api/xploit/learner/drqv2/#actor","title":"Actor","text":"<p>source <pre><code>Actor(\naction_space: Space, feature_dim: int = 64, hidden_dim: int = 1024\n)\n</code></pre></p> <p>Actor network</p> <p>Args</p> <ul> <li>action_space  : Action space of the environment.</li> <li>features_dim  : Number of features accepted.</li> <li>hidden_dim  : Number of units per hidden layer.</li> </ul> <p>Returns</p> <p>Actor network.</p> <p>Methods:</p>"},{"location":"api/xploit/learner/drqv2/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#critic","title":"Critic","text":"<p>source <pre><code>Critic(\naction_space: Space, feature_dim: int = 64, hidden_dim: int = 1024\n)\n</code></pre></p> <p>Critic network</p> <p>Args</p> <ul> <li>action_space  : Action space of the environment.</li> <li>features_dim  : Number of features accepted.</li> <li>hidden_dim  : Number of units per hidden layer.</li> </ul> <p>Returns</p> <p>Critic network.</p> <p>Methods:</p>"},{"location":"api/xploit/learner/drqv2/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor, action: Tensor\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#drqv2agent","title":"DrQv2Agent","text":"<p>source <pre><code>DrQv2Agent(\nobservation_space: Space, action_space: Space, device: torch.device = 'cuda',\nfeature_dim: int = 50, hidden_dim: int = 1024, lr: float = 0.0001,\ncritic_target_tau: float = 0.01, num_expl_steps: int = 2000,\nupdate_every_steps: int = 2, stddev_schedule: str = 'linear(1.0, 0.1, 100000)',\nstddev_clip: float = 0.3\n)\n</code></pre></p> <p>Learner for continuous control tasks. Current learner: DrQ-v2 Paper: Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning Link: https://openreview.net/pdf?id=_SJ-_yyes8</p> <p>Args</p> <ul> <li>obs_space  : The observation shape of the environment.</li> <li>action_shape  : The action shape of the environment.</li> <li>feature_dim  : Number of features extracted.</li> <li>hidden_dim  : The size of the hidden layers.</li> <li>lr  : The learning rate.</li> <li>critic_target_tau  : The critic Q-function soft-update rate.</li> <li>update_every_steps  : The agent update frequency.</li> <li>num_expl_steps  : The exploration steps.</li> <li>stddev_schedule  : The exploration std schedule.</li> <li>stddev_clip  : The exploration std clip range.</li> </ul> <p>Returns</p> <p>Agent instance.</p> <p>Methods:</p>"},{"location":"api/xploit/learner/drqv2/#train","title":".train","text":"<p>source <pre><code>.train(\ntraining = True\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#set_encoder","title":".set_encoder","text":"<p>source <pre><code>.set_encoder(\nencoder\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#set_dist","title":".set_dist","text":"<p>source <pre><code>.set_dist(\ndist\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#set_aug","title":".set_aug","text":"<p>source <pre><code>.set_aug(\naug\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#set_irs","title":".set_irs","text":"<p>source <pre><code>.set_irs(\nirs\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#act","title":".act","text":"<p>source <pre><code>.act(\nobs: ndarray, training: bool = True, step: int = 0\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#update","title":".update","text":"<p>source <pre><code>.update(\nreplay_iter: DataLoader, step: int = 0\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#update_critic","title":".update_critic","text":"<p>source <pre><code>.update_critic(\nobs: Tensor, action: Tensor, reward: Tensor, discount: Tensor, next_obs,\nstep: int\n)\n</code></pre></p>"},{"location":"api/xploit/learner/drqv2/#update_actor","title":".update_actor","text":"<p>source <pre><code>.update_actor(\nobs: Tensor, step: int\n)\n</code></pre></p>"},{"location":"api/xploit/learner/ppg/","title":"PPGAgent","text":""},{"location":"api/xploit/learner/ppg/#ppgagent","title":"PPGAgent","text":"<p>source </p>"},{"location":"api/xploit/storage/nstep_replay_buffer/","title":"NStepReplayBuffer","text":""},{"location":"api/xploit/storage/nstep_replay_buffer/#nstepreplaybuffer","title":"NStepReplayBuffer","text":"<p>source <pre><code>NStepReplayBuffer(\nbuffer_size: int, batch_size: int, num_workers: int, pin_memory: bool,\nn_step: int = 2, discount: float = 0.99, fetch_every: int = 1000,\nsave_snapshot: bool = False\n)\n</code></pre></p> <p>Replay buffer for off-policy algorithms (N-step returns supported).</p> <p>Args</p> <ul> <li>buffer_size  : Max number of element in the buffer.</li> <li>batch_size  : Number of samples per batch to load.</li> <li>num_workers  : Subprocesses to use for data loading.</li> <li>pin_memory  : Copy Tensors into device/CUDA pinned memory before returning them.</li> <li>n_step  : The number of transitions to consider when computing n-step returns</li> <li>discount  : The discount factor for future rewards.</li> <li>fetch_every  : Loading interval.</li> <li>save_snapshot  : Save loaded file or not.</li> </ul> <p>Returns</p> <p>N-step replay buffer.</p> <p>Methods:</p>"},{"location":"api/xploit/storage/nstep_replay_buffer/#add","title":".add","text":"<p>source <pre><code>.add(\nobservation: Any, action: Any, reward: float, done: float, info: Any\n)\n</code></pre></p>"},{"location":"api/xploit/storage/prioritized_replay_buffer/","title":"PrioritizedReplayBuffer","text":""},{"location":"api/xploit/storage/prioritized_replay_buffer/#prioritizedreplaybuffer","title":"PrioritizedReplayBuffer","text":"<p>source <pre><code>PrioritizedReplayBuffer(\nbuffer_size, alpha = 0.6, beta = 0.4, beta_schedule = None, epsilon = 1e-06\n)\n</code></pre></p> <p>Methods:</p>"},{"location":"api/xploit/storage/prioritized_replay_buffer/#add","title":".add","text":"<p>source <pre><code>.add(\nstate, action, reward, next_state, done\n)\n</code></pre></p>"},{"location":"api/xploit/storage/prioritized_replay_buffer/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nbatch_size\n)\n</code></pre></p>"},{"location":"api/xploit/storage/prioritized_replay_buffer/#update_priorities","title":".update_priorities","text":"<p>source <pre><code>.update_priorities(\nindices, priorities\n)\n</code></pre></p>"},{"location":"api/xploit/storage/vanilla_replay_buffer/","title":"VanillaReplayBuffer","text":""},{"location":"api/xploit/storage/vanilla_replay_buffer/#replaybufferstorage","title":"ReplayBufferStorage","text":"<p>source <pre><code>ReplayBufferStorage(\nreplay_dir: Path\n)\n</code></pre></p> <p>Storage collected experiences to local files.</p> <p>Args</p> <ul> <li>replay_dir  : save directory.</li> </ul> <p>Returns</p> <p>Storage instance.</p> <p>Methods:</p>"},{"location":"api/xploit/storage/vanilla_replay_buffer/#num_episodes","title":".num_episodes","text":"<p>source <pre><code>.num_episodes()\n</code></pre></p>"},{"location":"api/xploit/storage/vanilla_replay_buffer/#num_transitions","title":".num_transitions","text":"<p>source <pre><code>.num_transitions()\n</code></pre></p>"},{"location":"api/xploit/storage/vanilla_replay_buffer/#add","title":".add","text":"<p>source <pre><code>.add(\nobs: Any, action: Any, reward: float, done: bool, info: Any, use_discount: bool\n)\n</code></pre></p>"},{"location":"api/xploit/storage/vanilla_replay_buffer/#vanillareplaybuffer","title":"VanillaReplayBuffer","text":"<p>source <pre><code>VanillaReplayBuffer(\nbuffer_size: int, num_workers: int, discount: float = 0.99, fetch_every: int = 1000,\nsave_snapshot: bool = False\n)\n</code></pre></p> <p>Vanilla replay buffer for off-policy algorithms.</p> <p>Args</p> <ul> <li>buffer_size  : Max number of element in the buffer.</li> <li>num_workers  : Subprocesses to use for data loading.</li> <li>discount  : The discount factor for future rewards.</li> <li>fetch_every  : Loading interval.</li> <li>save_snapshot  : Save loaded file or not.</li> </ul> <p>Returns</p> <p>Vanilla replay buffer.</p> <p>Methods:</p>"},{"location":"api/xploit/storage/vanilla_replay_buffer/#add_1","title":".add","text":"<p>source <pre><code>.add(\nobservation: Any, action: Any, reward: float, done: float, info: Any\n)\n</code></pre></p>"},{"location":"api/xplore/augmentation/base/","title":"Base","text":""},{"location":"api/xplore/augmentation/base/#baseaugmentation","title":"BaseAugmentation","text":"<p>source </p> <p>Base class of augmentation.</p>"},{"location":"api/xplore/augmentation/random_crop/","title":"RandomCrop","text":""},{"location":"api/xplore/augmentation/random_crop/#randomcrop","title":"RandomCrop","text":"<p>source <pre><code>RandomCrop(\npad: int, out: int\n)\n</code></pre></p> <p>Random crop operation for processing image-based observations.</p> <p>Args</p> <ul> <li>pad  : Padding size.</li> <li>out  : Desired output size.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api/xplore/augmentation/random_crop/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/augmentation/random_shift/","title":"RandomShift","text":""},{"location":"api/xplore/augmentation/random_shift/#randomshift","title":"RandomShift","text":"<p>source <pre><code>RandomShift(\npad: int = 4\n)\n</code></pre></p> <p>Random shift operation for processing image-based observations.</p> <p>Args</p> <ul> <li>pad  : Padding size.</li> </ul> <p>Returns</p> <p>Augmented images.</p> <p>Methods:</p>"},{"location":"api/xplore/augmentation/random_shift/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nx: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/distribution/base/","title":"Base","text":""},{"location":"api/xplore/distribution/base/#basedistribution","title":"BaseDistribution","text":"<p>source <pre><code>BaseDistribution(\nmu: Tensor, sigma: Tensor, low: float = -1.0, high: float = 1.0, eps: float = 1e-06\n)\n</code></pre></p> <p>Base class of distribution.</p> <p>Args</p> <ul> <li>mu  : Mean of the distribution.</li> <li>sigma  : Standard deviation of the distribution.</li> <li>low  : Lower bound for action range.</li> <li>high  : Upper bound for action range.</li> <li>eps  : A constant for clamping.</li> </ul> <p>Returns</p> <p>Base distribution instance.</p> <p>Methods:</p>"},{"location":"api/xplore/distribution/base/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nclip: float = None, sample_shape = torch.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample</p> <p>Args</p> <ul> <li>clip  : Range for noise truncation operation.</li> <li>sample_shape  : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api/xplore/distribution/ornstein_uhlenbeck/","title":"OrnsteinUhlenbeck","text":""},{"location":"api/xplore/distribution/ornstein_uhlenbeck/#ornsteinuhlenbeck","title":"OrnsteinUhlenbeck","text":"<p>source <pre><code>OrnsteinUhlenbeck(\nmu: Tensor, sigma: Tensor, low: float = -1.0, high: float = 1.0, eps: float = 1e-06,\ntheta: float = 0.15, dt: float = 0.01, initial_noise: Optional[Tensor] = None\n)\n</code></pre></p> <p>Ornstein Uhlenbeck action noise.  Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab</p> <p>Args</p> <ul> <li>mu  : Mean of the distribution.</li> <li>sigma  : Standard deviation of the distribution.</li> <li>low  : Lower bound for action range.</li> <li>high  : Upper bound for action range.</li> <li>eps  : A constant for clamping.</li> <li>theta  : Rate of mean reversion.</li> <li>dt  : Timestep for the noise.</li> <li>initial_noise  : Initial value for the noise output, (if None: 0)</li> </ul> <p>Returns</p> <p>Ornstein-Uhlenbeck noise instance.</p> <p>Methods:</p>"},{"location":"api/xplore/distribution/ornstein_uhlenbeck/#reset","title":".reset","text":"<p>source <pre><code>.reset()\n</code></pre></p> <p>Reset the Ornstein Uhlenbeck noise, to the initial position</p>"},{"location":"api/xplore/distribution/ornstein_uhlenbeck/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nclip: float = None, sample_shape = torch.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample</p> <p>Args</p> <ul> <li>clip  : Range for noise truncation operation.</li> <li>sample_shape  : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api/xplore/distribution/truncated_normal/","title":"TruncatedNormal","text":""},{"location":"api/xplore/distribution/truncated_normal/#truncatednormal","title":"TruncatedNormal","text":"<p>source <pre><code>TruncatedNormal(\nmu: Tensor, sigma: Tensor, low: float = -1.0, high: float = 1.0, eps: float = 1e-06\n)\n</code></pre></p> <p>Truncated normal distribution for sampling noise.</p> <p>Args</p> <ul> <li>mu  : Mean of the distribution.</li> <li>sigma  : Standard deviation of the distribution.</li> <li>low  : Lower bound for action range.</li> <li>high  : Upper bound for action range.</li> <li>eps  : A constant for clamping.</li> </ul> <p>Returns</p> <p>Truncated normal distribution instance.</p> <p>Methods:</p>"},{"location":"api/xplore/distribution/truncated_normal/#sample","title":".sample","text":"<p>source <pre><code>.sample(\nclip: float = None, sample_shape = torch.Size()\n)\n</code></pre></p> <p>Generates a sample_shape shaped sample</p> <p>Args</p> <ul> <li>clip  : Range for noise truncation operation.</li> <li>sample_shape  : The size of the sample to be drawn.</li> </ul> <p>Returns</p> <p>A sample_shape shaped sample.</p>"},{"location":"api/xplore/reward/base/","title":"Base","text":""},{"location":"api/xplore/reward/base/#baseintrinsicrewardmodule","title":"BaseIntrinsicRewardModule","text":"<p>source <pre><code>BaseIntrinsicRewardModule(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float\n)\n</code></pre></p> <p>Base class of intrinsic reward module.</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> </ul> <p>Returns</p> <p>Instance of the base intrinsic reward module.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/base/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <p>Returns</p> <p>The intrinsic rewards.</p>"},{"location":"api/xplore/reward/base/#update","title":".update","text":"<p>source <pre><code>.update(\nrollouts: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <p>Returns</p> <p>None</p>"},{"location":"api/xplore/reward/girm/","title":"GIRM","text":""},{"location":"api/xplore/reward/girm/#girm","title":"GIRM","text":"<p>source <pre><code>GIRM(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int, lr: float, batch_size: int,\nlambd: float\n)\n</code></pre></p> <p>Intrinsic Reward Driven Imitation Learning via Generative Model (GIRM). See paper: http://proceedings.mlr.press/v119/yu20d/yu20d.pdf</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> <li>lr  : The learning rate of inverse and forward dynamics model.</li> <li>batch_size  : The batch size to train the dynamic models.</li> <li>lambd  : The weighting coefficient for combining actions.</li> </ul> <p>Returns</p> <p>Instance of GIRM.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/girm/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/girm/#update","title":".update","text":"<p>source <pre><code>.update(\nrollouts: Dict, lambda_recon: float = 1.0, lambda_action: float = 1.0,\nkld_loss_beta: float = 1.0\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>lambda_recon  : Weighting coefficient of the reconstruction loss.</li> <li>lambda_action  : Weighting coefficient of the action loss.</li> <li>kld_loss_beta  : Weighting coefficient of the divergence loss.</li> <p>Returns</p> <p>None</p>"},{"location":"api/xplore/reward/girm/#cnnencoder","title":"CnnEncoder","text":"<p>source <pre><code>CnnEncoder(\nobs_shape: Tuple\n)\n</code></pre></p> <p>CNN-based encoder of VAE.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> </ul> <p>Returns</p> <p>CNN-based encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/girm/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs, next_obs\n)\n</code></pre></p>"},{"location":"api/xplore/reward/girm/#cnndecoder","title":"CnnDecoder","text":"<p>source <pre><code>CnnDecoder(\nobs_shape: Tuple, action_dim: int, latent_dim: int\n)\n</code></pre></p> <p>CNN-based decoder of VAE.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> </ul> <p>Returns</p> <p>CNN-based decoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/girm/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nz, obs\n)\n</code></pre></p>"},{"location":"api/xplore/reward/girm/#mlpencoder","title":"MlpEncoder","text":"<p>source <pre><code>MlpEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>MLP-based encoder of VAE.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/girm/#forward_2","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor, next_obs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/girm/#mlpdecoder","title":"MlpDecoder","text":"<p>source <pre><code>MlpDecoder(\nobs_shape: Tuple, action_dim: int\n)\n</code></pre></p> <p>MLP-based decoder of VAE.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based decoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/girm/#forward_3","title":".forward","text":"<p>source <pre><code>.forward(\nz: Tensor, obs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/girm/#vae","title":"VAE","text":"<p>source <pre><code>VAE(\ndevice: torch.device, obs_shape: Tuple, latent_dim: int, action_dim: int\n)\n</code></pre></p> <p>Variational auto-encoder for reconstructing transition proces.</p> <p>Args</p> <ul> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> <li>action_dim  : The dimension of predicted actions.</li> </ul> <p>Methods:</p>"},{"location":"api/xplore/reward/girm/#reparameterize","title":".reparameterize","text":"<p>source <pre><code>.reparameterize(\nmu: Tensor, logvar: Tensor, device: torch.device, training: bool = True\n)\n</code></pre></p>"},{"location":"api/xplore/reward/girm/#forward_4","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor, next_obs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/icm/","title":"ICM","text":""},{"location":"api/xplore/reward/icm/#icm","title":"ICM","text":"<p>source <pre><code>ICM(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int, lr: float, batch_size: int\n)\n</code></pre></p> <p>Curiosity-Driven Exploration by Self-Supervised Prediction. See paper: http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> <li>lr  : The learning rate of inverse and forward dynamics model.</li> <li>batch_size  : The batch size to train the dynamic models.</li> </ul> <p>Returns</p> <p>Instance of ICM.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/icm/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/icm/#update","title":".update","text":"<p>source <pre><code>.update(\nrollouts: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <p>Returns</p> <p>None</p>"},{"location":"api/xplore/reward/icm/#cnnencoder","title":"CnnEncoder","text":"<p>source <pre><code>CnnEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Encoder for encoding image-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>CNN-based encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/icm/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor, next_obs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/icm/#inverseforwarddynamicsmodel","title":"InverseForwardDynamicsModel","text":"<p>source <pre><code>InverseForwardDynamicsModel(\nlatent_dim: int, action_dim: int\n)\n</code></pre></p> <p>Inverse-Forward model for reconstructing transition process.</p> <p>Args</p> <ul> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> <li>action_dim  : The dimension of predicted actions.</li> </ul> <p>Returns</p> <p>Model instance.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/icm/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor, action: Tensor, next_obs: Tensor, training: bool = True\n)\n</code></pre></p>"},{"location":"api/xplore/reward/ngu/","title":"NGU","text":""},{"location":"api/xplore/reward/ngu/#ngu","title":"NGU","text":"<p>source <pre><code>NGU(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int, lr: float, batch_size: int\n)\n</code></pre></p> <p>Never Give Up: Learning Directed Exploration Strategies (NGU). See paper: https://arxiv.org/pdf/2002.06038</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> <li>lr  : The learning rate of inverse and forward dynamics model.</li> <li>batch_size  : The batch size to train the dynamic models.</li> </ul> <p>Returns</p> <p>Instance of NGU.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/ngu/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/ngu/#update","title":".update","text":"<p>source <pre><code>.update(\nrollouts: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <p>Returns</p> <p>None</p>"},{"location":"api/xplore/reward/ngu/#pseudo_counts","title":".pseudo_counts","text":"<p>source <pre><code>.pseudo_counts(\nencoded_obs, k = 10, kernel_cluster_distance = 0.008, kernel_epsilon = 0.0001,\nc = 0.001, sm = 8\n)\n</code></pre></p>"},{"location":"api/xplore/reward/ngu/#cnnencoder","title":"CnnEncoder","text":"<p>source <pre><code>CnnEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Encoder for encoding image-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>CNN-based encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/ngu/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/ngu/#mlpencoder","title":"MlpEncoder","text":"<p>source <pre><code>MlpEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Encoder for encoding state-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/ngu/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/re3/","title":"RE3","text":""},{"location":"api/xplore/reward/re3/#re3","title":"RE3","text":"<p>source <pre><code>RE3(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int\n)\n</code></pre></p> <p>State Entropy Maximization with Random Encoders for Efficient Exploration (RE3).  See paper: http://proceedings.mlr.press/v139/seo21a/seo21a.pdf</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>Instance of RE3.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/re3/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int, k: int = 3, average_entropy: bool = False\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <li>k  : The k value for marking neighbors.</li> <li>average_entropy  : Use the average of entropy estimation.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/re3/#randommlpencoder","title":"RandomMlpEncoder","text":"<p>source <pre><code>RandomMlpEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding state-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/re3/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/re3/#randomcnnencoder","title":"RandomCnnEncoder","text":"<p>source <pre><code>RandomCnnEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding image-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>CNN-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/re3/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/revd/","title":"REVD","text":""},{"location":"api/xplore/reward/revd/#revd","title":"REVD","text":"<p>source <pre><code>REVD(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int\n)\n</code></pre></p> <p>Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning (REVD).  See paper: https://openreview.net/pdf?id=V2pw1VYMrDo</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>Instance of REVD.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/revd/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int, alpha: float = 0.5, k: int = 3,\naverage_divergence: bool = False\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <li>alpha  : The order of R\u00e9nyi divergence.</li> <li>k  : The k value for marking neighbors.</li> <li>average_divergence  : Use the average of divergence estimation.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/revd/#randommlpencoder","title":"RandomMlpEncoder","text":"<p>source <pre><code>RandomMlpEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding state-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/revd/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/revd/#randomcnnencoder","title":"RandomCnnEncoder","text":"<p>source <pre><code>RandomCnnEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding image-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>CNN-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/revd/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/ride/","title":"RIDE","text":""},{"location":"api/xplore/reward/ride/#ride","title":"RIDE","text":"<p>source <pre><code>RIDE(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int\n)\n</code></pre></p> <p>RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments. See paper: https://arxiv.org/pdf/2002.12292</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>Instance of RIDE.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/ride/#pseudo_counts","title":".pseudo_counts","text":"<p>source <pre><code>.pseudo_counts(\nsrc_feats, k = 10, kernel_cluster_distance = 0.008, kernel_epsilon = 0.0001,\nc = 0.001, sm = 8\n)\n</code></pre></p>"},{"location":"api/xplore/reward/ride/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/ride/#randomcnnencoder","title":"RandomCnnEncoder","text":"<p>source <pre><code>RandomCnnEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding image-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>CNN-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/ride/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/ride/#randommlpencoder","title":"RandomMlpEncoder","text":"<p>source <pre><code>RandomMlpEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding state-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/ride/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/rise/","title":"RISE","text":""},{"location":"api/xplore/reward/rise/#rise","title":"RISE","text":"<p>source <pre><code>RISE(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int\n)\n</code></pre></p> <p>R\u00e9nyi State Entropy Maximization for Exploration Acceleration in Reinforcement Learning (RISE).  See paper: https://ieeexplore.ieee.org/abstract/document/9802917/</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>Instance of RISE.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/rise/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int, alpha: float = 0.5, k: int = 3,\naverage_entropy: bool = False\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <li>alpha  : The The order of R\u00e9nyi entropy.</li> <li>k  : The k value for marking neighbors.</li> <li>average_entropy  : Use the average of entropy estimation.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/rise/#randommlpencoder","title":"RandomMlpEncoder","text":"<p>source <pre><code>RandomMlpEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding state-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/rise/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/rise/#randomcnnencoder","title":"RandomCnnEncoder","text":"<p>source <pre><code>RandomCnnEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Random encoder for encoding image-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>CNN-based random encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/rise/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/rnd/","title":"RND","text":""},{"location":"api/xplore/reward/rnd/#rnd","title":"RND","text":"<p>source <pre><code>RND(\nobs_shape: Tuple, action_shape: Tuple, action_type: str, device: torch.device,\nbeta: float, kappa: float, latent_dim: int, lr: float, batch_size: int\n)\n</code></pre></p> <p>Exploration by Random Network Distillation (RND). See paper: https://arxiv.org/pdf/1810.12894.pdf</p> <p>Args</p> <ul> <li>obs_shape  : Data shape of observation.</li> <li>action_space  : Data shape of action.</li> <li>action_type  : Continuous or discrete action. \"cont\" or \"dis\".</li> <li>device  : Device (cpu, cuda, ...) on which the code should be run.</li> <li>beta  : The initial weighting coefficient of the intrinsic rewards.</li> <li>kappa  : The decay rate.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> <li>lr  : The learning rate of inverse and forward dynamics model.</li> <li>batch_size  : The batch size to train the dynamic models.</li> </ul> <p>Returns</p> <p>Instance of RND.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/rnd/#compute_irs","title":".compute_irs","text":"<p>source <pre><code>.compute_irs(\nrollouts: Dict, step: int\n)\n</code></pre></p> <p>Compute the intrinsic rewards using the collected observations.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <li>step  : The current time step.</li> <p>Returns</p> <p>The intrinsic rewards</p>"},{"location":"api/xplore/reward/rnd/#update","title":".update","text":"<p>source <pre><code>.update(\nrollouts: Dict\n)\n</code></pre></p> <p>Update the intrinsic reward module if necessary.</p> <p>Args</p> <ul> <li>rollouts  : The collected experiences. A python dict like      {observations (n_steps, n_envs, *obs_shape) ,     actions (n_steps, n_envs, action_shape) ,     rewards (n_steps, n_envs, 1) }. <p>Returns</p> <p>None</p>"},{"location":"api/xplore/reward/rnd/#cnnencoder","title":"CnnEncoder","text":"<p>source <pre><code>CnnEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Encoder for encoding image-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>CNN-based encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/rnd/#forward","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"api/xplore/reward/rnd/#mlpencoder","title":"MlpEncoder","text":"<p>source <pre><code>MlpEncoder(\nobs_shape: Tuple, latent_dim: int\n)\n</code></pre></p> <p>Encoder for encoding state-based observations.</p> <p>Args</p> <ul> <li>obs_shape  : The data shape of observations.</li> <li>latent_dim  : The dimension of encoding vectors of the observations.</li> </ul> <p>Returns</p> <p>MLP-based encoder.</p> <p>Methods:</p>"},{"location":"api/xplore/reward/rnd/#forward_1","title":".forward","text":"<p>source <pre><code>.forward(\nobs: Tensor\n)\n</code></pre></p>"},{"location":"overview/api_overview/","title":"Overview","text":"<ul> <li> <p>Xploit: Modules that focus on exploitation in RL.</p> <ul> <li> <p>Encoder: Neural nework-based encoder for processing observations.</p> <ul> <li>BaseEncoder</li> <li>VanillaCnnEncoder</li> <li>VanillaMlpEncoder</li> </ul> </li> <li> <p>Learner: Agent for interacting and learning.</p> <ul> <li>ContinuousLearner (DrQ-v2)</li> <li>DiscreteLearner (PPG)</li> </ul> </li> <li> <p>Storage: Buffer for storing collected experiences.</p> <ul> <li>VanillaReplayBuffer</li> <li>NStepReplayBuffer</li> <li>PrioritizedReplayBuffer</li> <li>VanillaRolloutBuffer</li> </ul> </li> </ul> </li> <li> <p>Xplore: Modules that focus on exploration in RL.</p> <ul> <li> <p>Augmentation: PyTorch.nn-like modules for observation augmentation.</p> <ul> <li>RandomCrop</li> <li>RandomFlip</li> <li>RandomShift</li> </ul> </li> <li> <p>Distribution: Distributions for sampling actions.</p> <ul> <li>TruncatedNormal</li> <li>OrnsteinUhlenbeck</li> </ul> </li> <li> <p>Reward: Intrinsic reward modules for enhancing exploration.</p> <ul> <li>ICM</li> <li>RND</li> <li>GIRM</li> <li>NGU</li> <li>RIDE</li> <li>RE3</li> <li>RISE</li> <li>REVD</li> </ul> </li> </ul> </li> <li> <p>Pre-training: Methods of pre-training in RL.</p> </li> <li> <p>Deployment: Methods of model deployment in RL.</p> </li> </ul>"}]}